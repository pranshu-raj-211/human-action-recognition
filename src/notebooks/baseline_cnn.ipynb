{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VA0qxouGEwZ",
        "outputId": "5cceeb8a-6cfc-4638-f9d6-a566f99c2a4d"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU\n",
        "!pip install keras-tuner\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFH8FshnGEwW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict\n",
        "import warnings\n",
        "\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import (\n",
        "    Conv1D,\n",
        "    MaxPooling1D,\n",
        "    LSTM,\n",
        "    Dense,\n",
        "    ReLU,\n",
        "    Dropout,\n",
        "    GlobalAveragePooling1D,\n",
        "    BatchNormalization,\n",
        ")\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from keras.metrics import Precision, Recall, AUC, F1Score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import keras_tuner\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "JgIpdwGoGEwa",
        "outputId": "05dcaca2-f697-4676-d562-29928fa0d39d"
      },
      "outputs": [],
      "source": [
        "test_filenames = [\n",
        "    \"S1-ADL4_sensors_data.txt\",\n",
        "    \"S1-ADL5_sensors_data.txt\",\n",
        "    \"S2-ADL4_sensors_data.txt\",\n",
        "    \"S2-ADL5_sensors_data.txt\",\n",
        "    \"S3-ADL4_sensors_data.txt\",\n",
        "    \"S3-ADL5_sensors_data.txt\",\n",
        "    \"S4-ADL4_sensors_data.txt\",\n",
        "    \"S4-ADL5_sensors_data.txt\",\n",
        "]\n",
        "\n",
        "config = {\n",
        "    \"objective\": \"CNN-baseline, used in Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition\",\n",
        "    \"architecture\": \"CNN combined with Dense layers\",\n",
        "    'method_name':'CNN_Dense',\n",
        "    \"dataset\": \"Opportunity\",\n",
        "    \"empty_fill_method\": \"drop\",\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"window_size\": 16,\n",
        "    \"epochs\": 15,\n",
        "    \"batch_size\": 128,\n",
        "    \"results_directory\": \"results\",\n",
        "    \"project_name\": \"opportunity\",\n",
        "    'num_classes':5,\n",
        "}\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"opportunity-sensors\",\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2v0KlezGEwa"
      },
      "outputs": [],
      "source": [
        "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "wandb_callbacks = [\n",
        "    WandbMetricsLogger(),\n",
        "    WandbModelCheckpoint(filepath=config.get('method_name')+\"_{{epoch:02d}}.keras\"),\n",
        "    # todo: define these callbacks elsewhere\n",
        "    # EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=\"best_model.keras\", save_best_only=True),\n",
        "]\n",
        "\n",
        "# add this callback to model callbacks when training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1bd3iyOGEwb",
        "outputId": "d0877906-a3b6-43f5-f43a-17f1fb308b65"
      },
      "outputs": [],
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "data_path = \"/content/drive/My Drive/opportunity_raw/\"\n",
        "config_path = \"/content/drive/My Drive/opportunity_config/\"\n",
        "os.listdir(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZhVIU80GEwb"
      },
      "outputs": [],
      "source": [
        "def load_variable_names(filename):\n",
        "    with open(filename, \"r\") as file:\n",
        "        names = yaml.safe_load(file)\n",
        "    return names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79hXwXB4GEwb"
      },
      "outputs": [],
      "source": [
        "column_names = load_variable_names(\n",
        "    os.path.join(config_path, \"unique_column_names.yaml\")\n",
        ")\n",
        "locomotion_set = load_variable_names(os.path.join(config_path, \"locomotion_set.yaml\"))\n",
        "body_features = load_variable_names(os.path.join(config_path, \"body_features.yaml\"))\n",
        "assert type(locomotion_set) == list, \"Yaml not loaded correctly\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_XbNDLFJjWk"
      },
      "source": [
        "Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29rZ2C7SJiz-"
      },
      "outputs": [],
      "source": [
        "def get_file(filepath: str):\n",
        "    \"\"\"\n",
        "    Gets the data file with required subset columns (Locomotion set used here).\"\"\"\n",
        "    df = pd.read_csv(filepath, header=None, names=column_names, delimiter=\" \")\n",
        "    return df[locomotion_set]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX1IZj3zJvez"
      },
      "outputs": [],
      "source": [
        "def remove_missing_values(df: pd.DataFrame, method=\"linear\", order=None):\n",
        "    \"\"\"\n",
        "    Handling missing values in the dataset.\n",
        "    Specify method to handle the missing values in method parameter.\n",
        "\n",
        "    Method value being\n",
        "        linear does linear interpolation, with extrapolation for edge cases.\n",
        "        drop drops the rows containing missing values.\n",
        "        spline performs a spline interpolation with the specified order parameter.\n",
        "\n",
        "    Params:\n",
        "    - df (Pandas Dataframe): Dataframe from which missing values need to be removed.\n",
        "    - method (str): Method used to remove missing values (one of drop, linear or spline).\n",
        "    - order ()\n",
        "\n",
        "    Returns:\n",
        "    - A pandas dataframe object having no missing values.\n",
        "    \"\"\"\n",
        "    # todo: add method for removing missing rows entirely\n",
        "    # todo allow interpolation only if certain number of columns have missing values\n",
        "    if not df.empty:\n",
        "        if method == \"drop\":\n",
        "            return df.dropna(axis=0)\n",
        "        if method == \"spline\":\n",
        "            return df.interpolate(method=method, order=order)\n",
        "        return df.interpolate(method=method, limit_direction=\"both\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOH8S2w8J5JH"
      },
      "outputs": [],
      "source": [
        "def apply_sliding_window_combine(\n",
        "    filepath: str, target_var: str = \"Locomotion\", window_size=16, overlap=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Apply sliding window transforms to features and target of the given file.\n",
        "\n",
        "    Params:\n",
        "    - filepath (str): Path of the csv file that data is contained in.\n",
        "    - target_var (str): Target variable chosen for the given csv file.\n",
        "    - window_size (int): Size of one sliding window for the transform.\n",
        "    - overlap (float): Percentage overlap between two consecutive sliding windows.\n",
        "\n",
        "    Returns:\n",
        "    - A pandas dataframe object with shape (samples, window_size, features).\n",
        "    - A pandas Series object for the specified target variable.\n",
        "    \"\"\"\n",
        "    # todo: error handling for no or multiple target variables.\n",
        "    # todo: allow alternative saving as a pandas dataframe.\n",
        "\n",
        "    if target_var is None or type(target_var) != str:\n",
        "        raise TypeError(\n",
        "            \"Unexpected target variable passed, check for none or multiple target\"\n",
        "        )\n",
        "    shift_by = int(window_size * (1 - overlap))\n",
        "    df = get_file(filepath)\n",
        "    df = remove_missing_values(df, method=config.get('empty_fill_method'))\n",
        "    window_size = config.get(\"window_size\")\n",
        "    if window_size <= 1:\n",
        "        raise ValueError(\"Window size cannot be less than 2, fix config\")\n",
        "\n",
        "    def sliding_window_processing(data, window_size, shift_by):\n",
        "        \"\"\"\n",
        "        Perform sliding window operations on a single column of a df, return as np array.\n",
        "        Shape of returned array is (samples, window_size, 1)\n",
        "        \"\"\"\n",
        "        start, end = 0, window_size\n",
        "        windows = []\n",
        "        while end <= len(data):\n",
        "            windows.append(data[start:end])\n",
        "            start += shift_by\n",
        "            end += shift_by\n",
        "        return np.array(windows)\n",
        "\n",
        "    sliding_windows = {\n",
        "        feature: sliding_window_processing(df[feature].values, window_size, shift_by)\n",
        "        for feature in body_features\n",
        "    }\n",
        "    target = sliding_window_processing(df[target_var].values, window_size, shift_by)\n",
        "    combined_data = np.stack(\n",
        "        [sliding_windows[feature] for feature in body_features], axis=-1\n",
        "    )\n",
        "\n",
        "    return combined_data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FIKlpplKFuY"
      },
      "outputs": [],
      "source": [
        "# test filenames according to opportunity challenge\n",
        "# one issue remains - subject wise training needs to be checked as well\n",
        "\n",
        "combined_Xtrain = []\n",
        "combined_train_target = []\n",
        "combined_Xtest = []\n",
        "combined_test_target = []\n",
        "\n",
        "for filepath in os.listdir(data_path):\n",
        "    if os.path.basename(filepath) in test_filenames:\n",
        "        # print(\"test\", filepath)\n",
        "        combined_test, combined_test_t = apply_sliding_window_combine(\n",
        "            os.path.join(data_path, filepath)\n",
        "        )\n",
        "        # print(combined_test.shape, combined_test_t.shape)\n",
        "        combined_test_target.append(combined_test_t)\n",
        "        combined_Xtest.append(combined_test)\n",
        "    else:\n",
        "        # print(\"train\", filepath)\n",
        "        combined_train, combined_train_t = apply_sliding_window_combine(\n",
        "            os.path.join(data_path, filepath)\n",
        "        )\n",
        "        # print(combined_train.shape, combined_train_t.shape)\n",
        "        combined_train_target.append(combined_train_t)\n",
        "        combined_Xtrain.append(combined_train)\n",
        "\n",
        "\n",
        "# print(combined_Xtrain[0].shape, combined_Xtrain[1].shape)\n",
        "final_combined_train = np.concatenate(combined_Xtrain)\n",
        "final_combined_test = np.concatenate(combined_Xtest)\n",
        "final_combined_train_target = np.concatenate(combined_train_target)\n",
        "final_combined_test_target = np.concatenate(combined_test_target)\n",
        "\n",
        "X_train = np.array(final_combined_train)\n",
        "X_test = np.array(final_combined_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIM1Fi70Lk2S"
      },
      "outputs": [],
      "source": [
        "def majority_voting_labels(y_train):\n",
        "    y_train_adjusted = np.where(y_train > 2, y_train - 1, y_train)\n",
        "    y_train_majority_voted = np.apply_along_axis(\n",
        "        lambda x: np.bincount(x).argmax(), 1, y_train_adjusted\n",
        "    )\n",
        "\n",
        "    return y_train_majority_voted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO2hW_K8MDZq"
      },
      "outputs": [],
      "source": [
        "y_train_majority = majority_voting_labels(final_combined_train_target)\n",
        "y_test_majority = majority_voting_labels(final_combined_test_target)\n",
        "\n",
        "y_train_majority_encoded = to_categorical(y_train_majority)\n",
        "y_test_majority_encoded = to_categorical(y_test_majority)\n",
        "\n",
        "assert y_train_majority_encoded.shape[1] == 5, \"Label Encoding is incorrect\"\n",
        "assert not np.any(np.isnan(y_train_majority_encoded)), \"y_train contains NaN values\"\n",
        "assert not np.any(np.isnan(X_train)), \"X_train still has missing values\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "fTYjHRDxMcO9",
        "outputId": "d50975be-12a1-4d0d-a1c1-2dfaa493dfee"
      },
      "outputs": [],
      "source": [
        "y_train_classes, y_train_counts = np.unique(y_train_majority, return_counts=True)\n",
        "y_test_classes, y_test_counts = np.unique(y_test_majority, return_counts=True)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(y_train_classes, y_train_counts, color=\"blue\", alpha=0.7)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Class Distribution in Training Data\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(y_test_classes, y_test_counts, color=\"green\", alpha=0.7)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Class Distribution in Test Data\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbp9mkDyUXP7",
        "outputId": "97d73a49-b7cc-4e66-8db8-a1219dee26a3"
      },
      "outputs": [],
      "source": [
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH43ExD4GEwc"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    input_shape = (config.get('window_size'), 77)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, input_shape=input_shape, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(config.get('num_classes'), activation=\"softmax\"))\n",
        "    optimizer = Adam(\n",
        "        learning_rate=config.get('learning_rate'),\n",
        "        # clipnorm to prevent exploding gradients\n",
        "        clipnorm=1.0,\n",
        "        decay=1e-6,\n",
        "        amsgrad=True,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\", Precision(), Recall(), AUC(), F1Score(average='weighted')],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gutU2kmJZlc"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9QykjsQ_mrX",
        "outputId": "73b83359-78ca-4735-ebb5-3128338fb322"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train_majority_encoded, epochs=config.get('epochs'), batch_size=config.get('batch_size'), validation_split=config.get('validation_split'),callbacks=wandb_callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrWm4T32GEwd"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    results = model.evaluate(X_test, y_test, verbose=0)\n",
        "    metrics_names = model.metrics_names\n",
        "\n",
        "    for name, value in zip(metrics_names, results):\n",
        "        print(f'{name}: {value}')\n",
        "\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    class_report = classification_report(y_true, y_pred)\n",
        "    print('Classification Report:')\n",
        "    print(class_report)\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print('Confusion Matrix:')\n",
        "    print(cm)\n",
        "    \n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_prob = model.predict(X_test)\n",
        "\n",
        "# Step 2: Convert the predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "y_true = np.argmax(y_test_majority_encoded, axis=1)\n",
        "\n",
        "# Step 3: Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Optional: Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL_1v8Pa_mrX",
        "outputId": "a61a5386-dc03-4a80-b83d-542929f4a916"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test_majority_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U1XrHt_GpRg",
        "outputId": "9dfab926-ea50-422f-fa33-b388b7073830"
      },
      "outputs": [],
      "source": [
        "best_model = keras.models.load_model('best_model.keras')\n",
        "\n",
        "best_model.evaluate(X_test, y_test_majority_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFV9DCbzGEwc"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X_val, y_val, epochs=10, batch_size=64):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuner not initialized for this notebook, need to change model function for doing so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vwu0LuFKIeyO"
      },
      "outputs": [],
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    overwrite=True,\n",
        "    directory=\"results\",\n",
        "    project_name=\"opportunity\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "txtYfPkqMssi",
        "outputId": "523ac362-abf0-4ddc-f9b1-cecf564744ae"
      },
      "outputs": [],
      "source": [
        "tuner.search(\n",
        "    X_train,\n",
        "    y_train_majority_encoded,\n",
        "    epochs=config.get(\"epochs\"),\n",
        "    validation_split=0.2,\n",
        "    callbacks=wandb_callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YS21nDdJVKw"
      },
      "outputs": [],
      "source": [
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnKwqrcuNFGn"
      },
      "outputs": [],
      "source": [
        "models = tuner.get_best_models(num_models=2)\n",
        "best_model = models[0]\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CkwnuBjNFqL"
      },
      "outputs": [],
      "source": [
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GM80BEqGEwd"
      },
      "outputs": [],
      "source": [
        "def convert_history_to_dict(history) -> Dict:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiLn7KcdGEwd"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(history_dict):\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
