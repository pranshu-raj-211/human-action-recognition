{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VA0qxouGEwZ"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU\n",
        "!pip install keras-tuner\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFH8FshnGEwW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import (\n",
        "    Conv1D,\n",
        "    MaxPooling1D,\n",
        "    LSTM,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    GlobalAveragePooling1D,\n",
        "    BatchNormalization,\n",
        "    Flatten,\n",
        ")\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
        "from keras.utils import to_categorical\n",
        "from keras.metrics import Precision, Recall, AUC\n",
        "\n",
        "import keras_tuner\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgIpdwGoGEwa"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project=\"opportunity-sensors\",\n",
        "    config={\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"architecture\": \"CNN-LSTM\",\n",
        "        \"dataset\": \"Opportunity\",\n",
        "        \"epochs\": 10,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2v0KlezGEwa"
      },
      "outputs": [],
      "source": [
        "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "\n",
        "wandb_callbacks = [\n",
        "    WandbMetricsLogger(),\n",
        "    WandbModelCheckpoint(filepath=\"cnn_lstm{epoch:02d}.keras\"),\n",
        "    # todo: define these callbacks elsewhere\n",
        "    # EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
        "    # ModelCheckpoint(filepath=\"cnn_lstm{epoch:02d}\", save_best_only=True),\n",
        "]\n",
        "\n",
        "# add this callback to model callbacks when training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1bd3iyOGEwb"
      },
      "outputs": [],
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "data_path = \"/content/drive/My Drive/opportunity_raw/\"\n",
        "config_path = \"/content/drive/My Drive/opportunity_config/\"\n",
        "os.listdir(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZhVIU80GEwb"
      },
      "outputs": [],
      "source": [
        "def load_variable_names(filename):\n",
        "    with open(filename, \"r\") as file:\n",
        "        names = yaml.safe_load(file)\n",
        "    return names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79hXwXB4GEwb"
      },
      "outputs": [],
      "source": [
        "column_names = load_variable_names(\n",
        "    os.path.join(config_path, \"unique_column_names.yaml\")\n",
        ")\n",
        "locomotion_set = load_variable_names(os.path.join(config_path, \"locomotion_set.yaml\"))\n",
        "body_features = load_variable_names(os.path.join(config_path, \"body_features.yaml\"))\n",
        "assert type(locomotion_set) == list, \"Yaml not loaded correctly\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_XbNDLFJjWk"
      },
      "source": [
        "Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29rZ2C7SJiz-"
      },
      "outputs": [],
      "source": [
        "def get_file(filepath: str):\n",
        "    \"\"\"\n",
        "    Gets the data file with required subset columns (Locomotion set used here).\"\"\"\n",
        "    df = pd.read_csv(filepath, header=None, names=column_names, delimiter=\" \")\n",
        "    return df[locomotion_set]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX1IZj3zJvez"
      },
      "outputs": [],
      "source": [
        "def remove_missing_values(df: pd.DataFrame, method=\"linear\", order=None):\n",
        "    \"\"\"\n",
        "    Interpolation for missing values\n",
        "    \"\"\"\n",
        "    # todo: add method for removing missing rows entirely\n",
        "    # todo allow interpolation only if certain number of columns have missing values\n",
        "    if not df.empty:\n",
        "        if method == \"spline\":\n",
        "            return df.interpolate(method=method, order=order)\n",
        "        return df.interpolate(method=method, limit_direction=\"both\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOH8S2w8J5JH"
      },
      "outputs": [],
      "source": [
        "def apply_sliding_window_combine(\n",
        "    filepath: str, target_var: str = \"Locomotion\", window_size=16, overlap=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Apply sliding window transforms to features and target of the given file.\n",
        "\n",
        "    Params:\n",
        "    - filepath (str): Path of the csv file that data is contained in.\n",
        "    - target_var (str): Target variable chosen for the given csv file.\n",
        "    - window_size (int): Size of one sliding window for the transform.\n",
        "    - overlap (float): Percentage overlap between two consecutive sliding windows.\n",
        "    \"\"\"\n",
        "    # todo: error handling for no or multiple target variables.\n",
        "    # todo: allow alternative saving as a pandas dataframe.\n",
        "    shift_by = int(window_size * (1 - overlap))\n",
        "    df = get_file(filepath)\n",
        "    df = remove_missing_values(df)\n",
        "    # print(df.isnull().any())\n",
        "\n",
        "    def sliding_window_processing(data, window_size, shift_by):\n",
        "        \"\"\"\n",
        "        Perform sliding window operations on a single column of a df, return as np array.\n",
        "        \"\"\"\n",
        "        start, end = 0, window_size\n",
        "        windows = []\n",
        "        while end <= len(data):\n",
        "            windows.append(data[start:end])\n",
        "            start += shift_by\n",
        "            end += shift_by\n",
        "        return np.array(windows)\n",
        "\n",
        "    sliding_windows = {\n",
        "        feature: sliding_window_processing(df[feature].values, window_size, shift_by)\n",
        "        for feature in body_features\n",
        "    }\n",
        "    target = sliding_window_processing(df[target_var].values, window_size, shift_by)\n",
        "    combined_data = np.stack(\n",
        "        [sliding_windows[feature] for feature in body_features], axis=-1\n",
        "    )\n",
        "\n",
        "    return combined_data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FIKlpplKFuY"
      },
      "outputs": [],
      "source": [
        "# test filenames according to opportunity challenge\n",
        "# one issue remains - subject wise training needs to be checked as well\n",
        "test_filenames = [\n",
        "    \"S1-ADL4_sensors_data.txt\",\n",
        "    \"S1-ADL5_sensors_data.txt\",\n",
        "    \"S2-ADL4_sensors_data.txt\",\n",
        "    \"S2-ADL5_sensors_data.txt\",\n",
        "    \"S3-ADL4_sensors_data.txt\",\n",
        "    \"S3-ADL5_sensors_data.txt\",\n",
        "    \"S4-ADL4_sensors_data.txt\",\n",
        "    \"S4-ADL5_sensors_data.txt\",\n",
        "]\n",
        "\n",
        "\n",
        "combined_Xtrain = []\n",
        "combined_train_target = []\n",
        "combined_Xtest = []\n",
        "combined_test_target = []\n",
        "\n",
        "\n",
        "for filepath in os.listdir(data_path):\n",
        "    if os.path.basename(filepath) in test_filenames:\n",
        "        print(\"test\", filepath)\n",
        "        combined_test, combined_test_t = apply_sliding_window_combine(\n",
        "            os.path.join(data_path, filepath)\n",
        "        )\n",
        "        print(combined_test.shape, combined_test_t.shape)\n",
        "        combined_test_target.append(combined_test_t)\n",
        "        combined_Xtest.append(combined_test)\n",
        "\n",
        "    else:\n",
        "        print(\"train\", filepath)\n",
        "        combined_train, combined_train_t = apply_sliding_window_combine(\n",
        "            os.path.join(data_path, filepath)\n",
        "        )\n",
        "        # print(combined_train.shape, combined_train_t.shape)\n",
        "        combined_train_target.append(combined_train_t)\n",
        "        combined_Xtrain.append(combined_train)\n",
        "\n",
        "\n",
        "print(combined_Xtrain[0].shape, combined_Xtrain[1].shape)\n",
        "final_combined_train = np.concatenate(combined_Xtrain)\n",
        "final_combined_test = np.concatenate(combined_Xtest)\n",
        "final_combined_train_target = np.concatenate(combined_train_target)\n",
        "final_combined_test_target = np.concatenate(combined_test_target)\n",
        "\n",
        "X_train = np.array(final_combined_train)\n",
        "X_test = np.array(final_combined_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIM1Fi70Lk2S"
      },
      "outputs": [],
      "source": [
        "def majority_voting_labels(y_train):\n",
        "    y_train_adjusted = np.where(y_train > 2, y_train - 1, y_train)\n",
        "    y_train_majority_voted = np.apply_along_axis(\n",
        "        lambda x: np.bincount(x).argmax(), 1, y_train_adjusted\n",
        "    )\n",
        "\n",
        "    return y_train_majority_voted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO2hW_K8MDZq"
      },
      "outputs": [],
      "source": [
        "y_train_majority = majority_voting_labels(final_combined_train_target)\n",
        "y_test_majority = majority_voting_labels(final_combined_test_target)\n",
        "\n",
        "y_train_majority_encoded = to_categorical(y_train_majority)\n",
        "y_test_majority_encoded = to_categorical(y_test_majority)\n",
        "\n",
        "assert y_train_majority_encoded.shape[1] == 5, \"Label Encoding is incorrect\"\n",
        "assert not np.any(np.isnan(y_train_majority_encoded)), \"y_train contains NaN values\"\n",
        "assert not np.any(np.isnan(X_train)), \"X_train still has missing values\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTYjHRDxMcO9"
      },
      "outputs": [],
      "source": [
        "y_train_classes, y_train_counts = np.unique(y_train_majority, return_counts=True)\n",
        "y_test_classes, y_test_counts = np.unique(y_test_majority, return_counts=True)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(y_train_classes, y_train_counts, color=\"blue\", alpha=0.7)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Class Distribution in Training Data\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(y_test_classes, y_test_counts, color=\"green\", alpha=0.7)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Class Distribution in Test Data\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbp9mkDyUXP7"
      },
      "outputs": [],
      "source": [
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH43ExD4GEwc"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "    input_shape = (16, 77)\n",
        "    num_classes = 5\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=32, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(LSTM(units=32, return_sequences=True))\n",
        "\n",
        "    model.add(\n",
        "        Conv1D(\n",
        "            filters=64,\n",
        "            kernel_size=hp.Int(\"kernel_size\", min_value=3, max_value=5, step=2),\n",
        "            strides=hp.Int(\"strides\", min_value=2, max_value=4, step=1),\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "    )\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, strides=1, activation=\"relu\"))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "\n",
        "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "    optimizer = Adam(\n",
        "        learning_rate=hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\", Precision(), Recall(), AUC()],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gutU2kmJZlc"
      },
      "outputs": [],
      "source": [
        "build_model(hp=keras_tuner.HyperParameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFV9DCbzGEwc"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X_val, y_val, epochs=10, batch_size=64):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vwu0LuFKIeyO"
      },
      "outputs": [],
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    overwrite=True,\n",
        "    directory=\"results\",\n",
        "    project_name=\"opportunity\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txtYfPkqMssi"
      },
      "outputs": [],
      "source": [
        "tuner.search(\n",
        "    X_train,\n",
        "    y_train_majority_encoded,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    callbacks=wandb_callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YS21nDdJVKw"
      },
      "outputs": [],
      "source": [
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnKwqrcuNFGn"
      },
      "outputs": [],
      "source": [
        "models = tuner.get_best_models(num_models=2)\n",
        "best_model = models[0]\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CkwnuBjNFqL"
      },
      "outputs": [],
      "source": [
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrWm4T32GEwd"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test, metrics):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GM80BEqGEwd"
      },
      "outputs": [],
      "source": [
        "def convert_history_to_dict(history) -> Dict:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiLn7KcdGEwd"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(history_dict):\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
